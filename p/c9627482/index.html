<!DOCTYPE html><html><head><meta charset="utf-8"><title>Keras 损失函数 | 曹耘豪的博客</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><link rel="alternate" href="/atom.xml" title="曹耘豪的博客" type="application/atom+xml"><link rel="icon" href="https://s1.ax1x.com/2018/06/09/CbgQqs.png"><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="header2" style="border-bottom:1px solid #ccc"><input type="checkbox" id="header-menu-input"><h1 style="text-align:left"><a href="/">曹耘豪的博客</a> <label for="header-menu-input"><svg class="op open" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" width="24" height="24" viewBox="0 0 50 50" fill="currentcolor" stroke="currentcolor"><path d="M 0 7.5 L 0 12.5 L 50 12.5 L 50 7.5 Z M 0 22.5 L 0 27.5 L 50 27.5 L 50 22.5 Z M 0 37.5 L 0 42.5 L 50 42.5 L 50 37.5 Z"></path></svg> <svg class="close" xmlns="http://www.w3.org/2000/svg" x="0px" y="0px" width="24" height="24" viewBox="0 0 30 30" style="fill:red"><path d="M 7 4 C 6.744125 4 6.4879687 4.0974687 6.2929688 4.2929688 L 4.2929688 6.2929688 C 3.9019687 6.6839688 3.9019687 7.3170313 4.2929688 7.7070312 L 11.585938 15 L 4.2929688 22.292969 C 3.9019687 22.683969 3.9019687 23.317031 4.2929688 23.707031 L 6.2929688 25.707031 C 6.6839688 26.098031 7.3170313 26.098031 7.7070312 25.707031 L 15 18.414062 L 22.292969 25.707031 C 22.682969 26.098031 23.317031 26.098031 23.707031 25.707031 L 25.707031 23.707031 C 26.098031 23.316031 26.098031 22.682969 25.707031 22.292969 L 18.414062 15 L 25.707031 7.7070312 C 26.098031 7.3170312 26.098031 6.6829688 25.707031 6.2929688 L 23.707031 4.2929688 C 23.316031 3.9019687 22.682969 3.9019687 22.292969 4.2929688 L 15 11.585938 L 7.7070312 4.2929688 C 7.5115312 4.0974687 7.255875 4 7 4 z"></path></svg></label> <input disabled id="local-search-input" class="input-text" type="search" placeholder="js未开启，无法搜索" aria-label="Search"></h1><div id="local-search-result"></div><div class="header-menu" for="header-menu-input"><aside id="sidebar"><div><div class="widget-wrap widget-category"><h3 class="widget-title">目录</h3><div class="widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/category/other/">其他</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/">技术</a><span class="category-list-count">137</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/category/technology/cpp/">C++</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/Go/">Go</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/java/">Java</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/python/">Python</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/Rust/">Rust</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/machine-learning/">机器学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/computer-network/">计算机网络</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/design-pattern/">设计模式</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/%E8%AF%B4%E8%AF%B4/">说说</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/category/technology/%E8%BF%90%E7%BB%B4/">运维</a><span class="category-list-count">13</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/category/algorithm/">算法</a><span class="category-list-count">8</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">标签</h3><div class="widget"><ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/" rel="tag">MySQL</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Protobuf/" rel="tag">Protobuf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spring-Cloud/" rel="tag">Spring Cloud</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feign/" rel="tag">Feign</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E8%AF%95/" rel="tag">面试</a><span class="tag-list-count">21</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" rel="tag">问题排查</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/React/" rel="tag">React</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nextjs/" rel="tag">Nextjs</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spring/" rel="tag">Spring</a><span class="tag-list-count">20</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jvm/" rel="tag">JVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%90%86/" rel="tag">代理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradle/" rel="tag">Gradle</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/" rel="tag">Maven</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/fastjson/" rel="tag">fastjson</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/" rel="tag">nginx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node/" rel="tag">node</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/npm/" rel="tag">npm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BC%93%E5%AD%98/" rel="tag">缓存</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Redis/" rel="tag">Redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mybatis/" rel="tag">Mybatis</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/" rel="tag">redis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/" rel="tag">Hive</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B5%8B%E8%AF%95/" rel="tag">测试</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/" rel="tag">HBase</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B9%B6%E5%8F%91/" rel="tag">并发</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/" rel="tag">响应式编程</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">其他</h3><div class="widget"><a href="/atom.xml">RSS Feed </a><a href="/resume" style="margin-left:8px">Resume</a></div></div></div></aside></div><script src="/js/search.js"></script><script type="text/javascript">(()=>{var e="local-search-input",a=(($input=document.getElementById(e)).disabled=!1,$input.placeholder="全站本地搜索","search.json"),a="/"+(a=0===a.length?"search.json":a);searchFunc(a,e,"local-search-result")})()</script></div><article id="post-c9627482" class="article article-type-post" itemscope itemprop="blogPost" style="position:relative"><header><h1 class="article-title" itemprop="name">Keras 损失函数</h1></header><div class="secondary-bg" style="border:1px solid gray;border-radius:4px"><ol class="toc-nav"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-nav-text">定义</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mean-squared-error%EF%BC%9A%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-mse"><span class="toc-nav-text">mean_squared_error：均方误差(mse)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mean-absolute-error-mae"><span class="toc-nav-text">mean_absolute_error (mae)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mean-absolute-percentage-error-mape"><span class="toc-nav-text">mean_absolute_percentage_error (mape)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mean-squared-logarithmic-error"><span class="toc-nav-text">mean_squared_logarithmic_error</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#squared-hinge"><span class="toc-nav-text">squared_hinge</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#hinge"><span class="toc-nav-text">hinge</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#categorical-hinge"><span class="toc-nav-text">categorical_hinge</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#logcosh"><span class="toc-nav-text">logcosh</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#categorical-crossentropy"><span class="toc-nav-text">categorical_crossentropy</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#sparse-categorical-crossentropy"><span class="toc-nav-text">sparse_categorical_crossentropy</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#binary-crossentropy"><span class="toc-nav-text">binary_crossentropy</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#kullback-leibler-divergence"><span class="toc-nav-text">kullback_leibler_divergence</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#poisson"><span class="toc-nav-text">poisson</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cosine-proximity"><span class="toc-nav-text">cosine_proximity</span></a></li></ol></div><span id="more"></span><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>损失函数是模型优化的目标，所以又叫目标函数、优化评分函数，在keras中，模型编译的参数loss指定了损失函数的类别，有两种指定方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mean_squared_error&#x27;</span>, optimizer=<span class="string">&#x27;sgd&#x27;</span>)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> losses</span><br><span class="line">model.<span class="built_in">compile</span>(loss=losses.mean_squared_error, optimizer=<span class="string">&#x27;sgd&#x27;</span>)</span><br></pre></td></tr></table></figure><p>你可以传递一个现有的损失函数名，或者一个TensorFlow&#x2F;Theano符号函数。 该符号函数为每个数据点返回一个标量，有以下两个参数:</p><ul><li>y_true: 真实标签. TensorFlow&#x2F;Theano张量</li><li>y_pred: 预测值. TensorFlow&#x2F;Theano张量，其shape与y_true相同</li></ul><p>实际的优化目标是所有数据点的输出数组的平均值。</p><!-- more --><h3 id="mean-squared-error：均方误差-mse"><a href="#mean-squared-error：均方误差-mse" class="headerlink" title="mean_squared_error：均方误差(mse)"></a>mean_squared_error：均方误差(mse)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(y_pred - y_true), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^&#123;n&#125;(y_&#123;true&#125;^&#123;(i)&#125;-y_&#123;pred&#125;^&#123;(i)&#125;)^2</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum_%7bi%3d1%7d%5e%7bn%7d(y_%7btrue%7d%5e%7b(i)%7d-y_%7bpred%7d%5e%7b(i)%7d)%5e2"><h3 id="mean-absolute-error-mae"><a href="#mean-absolute-error-mae" class="headerlink" title="mean_absolute_error (mae)"></a>mean_absolute_error (mae)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_absolute_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.<span class="built_in">abs</span>(y_pred - y_true), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum_&#123;i=1&#125;^&#123;n&#125;|(y_&#123;true&#125;^&#123;(i)&#125;-y_&#123;pred&#125;^&#123;(i)&#125;)|</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum_%7bi%3d1%7d%5e%7bn%7d%7c(y_%7btrue%7d%5e%7b(i)%7d-y_%7bpred%7d%5e%7b(i)%7d)%7c"><h3 id="mean-absolute-percentage-error-mape"><a href="#mean-absolute-percentage-error-mape" class="headerlink" title="mean_absolute_percentage_error (mape)"></a>mean_absolute_percentage_error (mape)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_absolute_percentage_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    diff = K.<span class="built_in">abs</span>((y_true - y_pred) / K.clip(K.<span class="built_in">abs</span>(y_true),</span><br><span class="line">                                            K.epsilon(),</span><br><span class="line">                                            <span class="literal">None</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">100.</span> * K.mean(diff, axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum^n_&#123;i=1&#125;|\frac &#123;y_&#123;true&#125;^&#123;(i)&#125;-y_&#123;pred&#125;^&#123;(i)&#125;&#125;&#123;y_&#123;true&#125;^&#123;(i)&#125;&#125;|\cdot 100</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7d%7c%5cfrac+%7by_%7btrue%7d%5e%7b(i)%7d-y_%7bpred%7d%5e%7b(i)%7d%7d%7by_%7btrue%7d%5e%7b(i)%7d%7d%7c%5ccdot+100"><h3 id="mean-squared-logarithmic-error"><a href="#mean-squared-logarithmic-error" class="headerlink" title="mean_squared_logarithmic_error"></a>mean_squared_logarithmic_error</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mean_squared_logarithmic_error</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    first_log = K.log(K.clip(y_pred, K.epsilon(), <span class="literal">None</span>) + <span class="number">1.</span>)</span><br><span class="line">    second_log = K.log(K.clip(y_true, K.epsilon(), <span class="literal">None</span>) + <span class="number">1.</span>)</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(first_log - second_log), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum^n_&#123;i=1&#125;(log(y_&#123;true&#125;^&#123;(i)&#125; +1)-log(y_&#123;pred&#125;^&#123;(i)&#125;+1))^2</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7d(log(y_%7btrue%7d%5e%7b(i)%7d+%2b1)-log(y_%7bpred%7d%5e%7b(i)%7d%2b1))%5e2"><h3 id="squared-hinge"><a href="#squared-hinge" class="headerlink" title="squared_hinge"></a>squared_hinge</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_hinge</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.square(K.maximum(<span class="number">1.</span> - y_true * y_pred, <span class="number">0.</span>)), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum^n_&#123;i=1&#125;(max(0,1-y_&#123;pred&#125;^&#123;(i)&#125; \cdot y_&#123;true&#125;^&#123;(i)&#125;))^2</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7d(max(0%2c1-y_%7bpred%7d%5e%7b(i)%7d+%5ccdot+y_%7btrue%7d%5e%7b(i)%7d))%5e2"><h3 id="hinge"><a href="#hinge" class="headerlink" title="hinge"></a>hinge</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hinge</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.maximum(<span class="number">1.</span> - y_true * y_pred, <span class="number">0.</span>), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum^n_&#123;i=1&#125;max(0,1-y_&#123;pred&#125;^&#123;(i)&#125;\cdot y_&#123;true&#125;^&#123;(i)&#125;)</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7dmax(0%2c1-y_%7bpred%7d%5e%7b(i)%7d%5ccdot+y_%7btrue%7d%5e%7b(i)%7d)"><h3 id="categorical-hinge"><a href="#categorical-hinge" class="headerlink" title="categorical_hinge"></a>categorical_hinge</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">categorical_hinge</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    pos = K.<span class="built_in">sum</span>(y_true * y_pred, axis=-<span class="number">1</span>)</span><br><span class="line">    neg = K.<span class="built_in">max</span>((<span class="number">1.</span> - y_true) * y_pred, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> K.maximum(<span class="number">0.</span>, neg - pos + <span class="number">1.</span>)</span><br></pre></td></tr></table></figure><h3 id="logcosh"><a href="#logcosh" class="headerlink" title="logcosh"></a>logcosh</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logcosh</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Logarithm of the hyperbolic cosine of the prediction error.</span></span><br><span class="line"><span class="string">    `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and</span></span><br><span class="line"><span class="string">    to `abs(x) - log(2)` for large `x`. This means that &#x27;logcosh&#x27; works mostly</span></span><br><span class="line"><span class="string">    like the mean squared error, but will not be so strongly affected by the</span></span><br><span class="line"><span class="string">    occasional wildly incorrect prediction.</span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        y_true: tensor of true targets.</span></span><br><span class="line"><span class="string">        y_pred: tensor of predicted targets.</span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        Tensor with one scalar loss entry per sample.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_logcosh</span>(<span class="params">x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + K.softplus(-<span class="number">2.</span> * x) - K.log(<span class="number">2.</span>)</span><br><span class="line">    <span class="keyword">return</span> K.mean(_logcosh(y_pred - y_true), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="categorical-crossentropy"><a href="#categorical-crossentropy" class="headerlink" title="categorical_crossentropy"></a>categorical_crossentropy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">categorical_crossentropy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.categorical_crossentropy(y_true, y_pred)</span><br></pre></td></tr></table></figure><p>注意: 当使用categorical_crossentropy损失时，你的目标值应该是分类格式 (即，如果你有10个类，每个样本的目标值应该是一个10维的向量，这个向量除了表示类别的那个索引为1，其他均为0)。 为了将 整数目标值 转换为 分类目标值，你可以使用Keras实用函数to_categorical：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line">categorical_labels = to_categorical(int_labels, num_classes=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="sparse-categorical-crossentropy"><a href="#sparse-categorical-crossentropy" class="headerlink" title="sparse_categorical_crossentropy"></a>sparse_categorical_crossentropy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sparse_categorical_crossentropy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.sparse_categorical_crossentropy(y_true, y_pred)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sparse_categorical_crossentropy</span>(<span class="params">target, output, from_logits=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Categorical crossentropy with integer targets.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        target: An integer tensor.</span></span><br><span class="line"><span class="string">        output: A tensor resulting from a softmax</span></span><br><span class="line"><span class="string">            (unless `from_logits` is True, in which</span></span><br><span class="line"><span class="string">            case `output` is expected to be the logits).</span></span><br><span class="line"><span class="string">        from_logits: Boolean, whether `output` is the</span></span><br><span class="line"><span class="string">            result of a softmax, or is a tensor of logits.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        Output tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Note: tf.nn.sparse_softmax_cross_entropy_with_logits</span></span><br><span class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</span><br><span class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</span><br><span class="line">        output = tf.log(output)</span><br><span class="line"></span><br><span class="line">    output_shape = output.get_shape()</span><br><span class="line">    targets = cast(flatten(target), <span class="string">&#x27;int64&#x27;</span>)</span><br><span class="line">    logits = tf.reshape(output, [-<span class="number">1</span>, <span class="built_in">int</span>(output_shape[-<span class="number">1</span>])])</span><br><span class="line">    res = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        labels=targets,</span><br><span class="line">        logits=logits)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(output_shape) &gt;= <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># if our output includes timestep dimension</span></span><br><span class="line">        <span class="comment"># or spatial dimensions we need to reshape</span></span><br><span class="line">        <span class="keyword">return</span> tf.reshape(res, tf.shape(output)[:-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h3 id="binary-crossentropy"><a href="#binary-crossentropy" class="headerlink" title="binary_crossentropy"></a>binary_crossentropy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_crossentropy</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.binary_crossentropy(y_true, y_pred), axis=-<span class="number">1</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">binary_crossentropy</span>(<span class="params">target, output, from_logits=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Arguments</span></span><br><span class="line"><span class="string">        target: A tensor with the same shape as `output`.</span></span><br><span class="line"><span class="string">        output: A tensor.</span></span><br><span class="line"><span class="string">        from_logits: Whether `output` is expected to be a logits tensor.</span></span><br><span class="line"><span class="string">            By default, we consider that `output`</span></span><br><span class="line"><span class="string">            encodes a probability distribution.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # Returns</span></span><br><span class="line"><span class="string">        A tensor.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Note: tf.nn.sigmoid_cross_entropy_with_logits</span></span><br><span class="line">    <span class="comment"># expects logits, Keras expects probabilities.</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> from_logits:</span><br><span class="line">        <span class="comment"># transform back to logits</span></span><br><span class="line">        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)</span><br><span class="line">        output = tf.clip_by_value(output, _epsilon, <span class="number">1</span> - _epsilon)</span><br><span class="line">        output = tf.log(output / (<span class="number">1</span> - output))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=target,logits=output)</span><br></pre></td></tr></table></figure><h3 id="kullback-leibler-divergence"><a href="#kullback-leibler-divergence" class="headerlink" title="kullback_leibler_divergence"></a>kullback_leibler_divergence</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kullback_leibler_divergence</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    y_true = K.clip(y_true, K.epsilon(), <span class="number">1</span>)</span><br><span class="line">    y_pred = K.clip(y_pred, K.epsilon(), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> K.<span class="built_in">sum</span>(y_true * K.log(y_true / y_pred), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="poisson"><a href="#poisson" class="headerlink" title="poisson"></a>poisson</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">poisson</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="keyword">return</span> K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=\frac&#123;1&#125;&#123;n&#125;\sum^n_&#123;i=1&#125;(y_&#123;pred&#125;^&#123;(i)&#125;-y_&#123;true&#125;^&#123;(i)&#125;\cdot log(y_&#123;pred&#125;^&#123;(i)&#125;))</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7d(y_%7bpred%7d%5e%7b(i)%7d-y_%7btrue%7d%5e%7b(i)%7d%5ccdot+log(y_%7bpred%7d%5e%7b(i)%7d))"><h3 id="cosine-proximity"><a href="#cosine-proximity" class="headerlink" title="cosine_proximity"></a>cosine_proximity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_proximity</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    y_true = K.l2_normalize(y_true, axis=-<span class="number">1</span>)</span><br><span class="line">    y_pred = K.l2_normalize(y_pred, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> -K.<span class="built_in">sum</span>(y_true * y_pred, axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>LaTeX：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L=-\frac&#123;\sum^n_&#123;i=1&#125;y_&#123;true&#125;^&#123;(i)&#125;\cdot y_&#123;pred&#125;^&#123;(i)&#125;&#125;&#123;\sqrt&#123;\sum^n_&#123;i=1&#125; (y_&#123;true&#125;^&#123;(i)&#125;)^2&#125;\cdot\sqrt&#123;\sum^n_&#123;i=1&#125;(y_&#123;pred&#125;^&#123;(i)&#125;)^2&#125;&#125;</span><br></pre></td></tr></table></figure><p>公式：</p><img src="https://www.zhihu.com/equation?tex=L%3d%5cfrac%7b1%7d%7bn%7d%5csum%5en_%7bi%3d1%7dmax(0%2c1-y_%7bpred%7d%5e%7b(i)%7d%5ccdot+y_%7btrue%7d%5e%7b(i)%7d)"><p>简写：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mse = MSE = mean_squared_error</span><br><span class="line">mae = MAE = mean_absolute_error</span><br><span class="line">mape = MAPE = mean_absolute_percentage_error</span><br><span class="line">msle = MSLE = mean_squared_logarithmic_error</span><br><span class="line">kld = KLD = kullback_leibler_divergence</span><br><span class="line">cosine = cosine_proximity</span><br></pre></td></tr></table></figure><p>参考: <a href="https://zhuanlan.zhihu.com/p/34667893">知乎</a></p><div style="border-top:2px solid #eee;height:1px;margin:8px 0"></div><div class="article-category"><svg class="i-tag" width="16px" height="16px" viewBox="0 0 24 24" fill="currentcolor" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M5.41959 3.23866C6.23018 3.05852 7.19557 3 8.312 3H9.92963C10.9327 3 11.8694 3.5013 12.4258 4.3359L13.2383 5.5547C13.4238 5.83288 13.736 6 14.0704 6H19.1258C20.7233 6 22.0181 7.26115 22.0029 8.8852C21.9847 10.8192 22 12.7539 22 14.688C22 15.8044 21.9415 16.7698 21.7613 17.5804C21.5787 18.4024 21.2579 19.1251 20.6915 19.6915C20.1251 20.2579 19.4024 20.5787 18.5804 20.7613C17.7698 20.9415 16.8044 21 15.688 21H8.312C7.19557 21 6.23018 20.9415 5.41959 20.7613C4.59764 20.5787 3.87488 20.2579 3.30848 19.6915C2.74209 19.1251 2.42133 18.4024 2.23866 17.5804C2.05852 16.7698 2 15.8044 2 14.688V9.312C2 8.19557 2.05852 7.23018 2.23866 6.41959C2.42133 5.59764 2.74209 4.87488 3.30848 4.30848C3.87488 3.74209 4.59764 3.42133 5.41959 3.23866ZM8.19103 13.8535C8.05868 14.449 8 15.2412 8 16.312V18C8 18.5523 7.55228 19 7 19C6.44772 19 6 18.5523 6 18V16.312C6 15.1956 6.05852 14.2302 6.23866 13.4196C6.42133 12.5976 6.74209 11.8749 7.30848 11.3085C7.87488 10.7421 8.59764 10.4213 9.41959 10.2387C10.2302 10.0585 11.1956 10 12.312 10H17.688H19C19.5523 10 20 10.4477 20 11C20 11.5523 19.5523 12 19 12H17.688H12.312C11.2412 12 10.449 12.0587 9.85349 12.191C9.26933 12.3209 8.9375 12.5079 8.72269 12.7227C8.50789 12.9375 8.32085 13.2693 8.19103 13.8535Z"/></svg> &nbsp; <a class="article-category-link" href="/category/technology/">技术</a>&nbsp;/&nbsp;<a class="article-category-link" href="/category/technology/machine-learning/">机器学习</a></div><div class="article-date"><time datetime="2018-05-23T10:00:00.000Z" itemprop="datePublished">2018-05-23</time></div></article></body></html>